## 背景
联邦服务的本意是跨越集群边界的Service，目前还没有标准的方式来实现跨越集群边界的服务，在[KEP-1645](https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/1645-multi-cluster-services-api)提案中就提出了两个CRD（ServiceExport和ServiceImport）来最小化配置跨集群的服务。

服务对于Pod内的应用程序来说是不透明，应用程序无法知道Service背后的Endpoint到底有多少个IP端点。除非用了etcd服务发现这种取代了k8s的Service做服务发现。但这里讨论的是跨集群的Service，不讨论微服务的服务发现实现方式。

为了更好地支持故障转义和升级时流量迁移时的临时业务支持，用户可能希望将一个服务分布在不同集群中。但这个实现的成本是巨大的。Multi Cluster Service API 就尝试标准化这一机制。

## KEP-1645
* ### 目标
    * 定义最简的CRD API来跨集群的服务发现和使用
        * 在一个集群中访问另一个集群定义的服务
        * 在一个集群中访问后端是多个集群的POD构成的的跨集群服务
    * 当一个跨集群服务被另一个集群使用时，使用体验和本地集群Service的使用体验要一致

* ### 名词术语
    * #### clusterset 
    一堆相互信任并互相分享服务的集群，集群成员之间能互相感知对方存在，并且能互联的。在每个集群中都存在的命名空间将被叫做集群空间。CRD API的实现程序需要维护成员关系
    * #### mcs-controller
    控制器被设计来同步跨集群服务到每个集群上，并维护跨集群服务的发现和连通。

    这世上可能有很多种控制器的实现。可能是单体程序，也可能是多个无中心的CRD控制器程序，也可能就是人肉维护的kubectl去同步跨集群服务。

    * #### cluster name 
    一个唯一的名字或者标示符（如UUID）这样的文本来标示一个集群，主要用于实现集群注册的时候区分不同集群。cluster name的值需要符合 [RFC-1123](https://tools.ietf.org/html/rfc1123) DNS规范，保持和k8s的metadata.name就行。

* ### CRD
    * 草案提议了两个CRD ，分别是 ServiceExport 和 ServiceImport 
    * #### ServiceExport
        * #### 解释
            用来定义哪个服务需要作为跨集群服务暴露到clusterset中。ServiceExport 必须被创建在同空间下面，最好名称和要暴露的Service名字也一样。并且在创建是设置好OwnerRef方便清理（除非不想在删除Service之后清理ServiceExport）。要暴露哪个集群的Service就给哪个集群创建ServiceExport对象。

            如果多个集群暴露了相同名称相同空间的Service，这些服务将会被作为一个跨集群服务，可以理解为负载均衡RoundRobin的多个后端。比如5个集群暴露了Service my-svc.my-ns，每一个引用的集群都将有一个名为 my-svc 的 ServiceImport 在 my-ns 空间内，并且关联上所有集群Service对应的Endpoints以实现后端对接流量。相应的 ServiceImport的字段（如ports, topology） 需要将每个暴露该服务的Endpoints吸纳进来并推给同空间内同名的 EndpointSlice 对象。

            EndpointSlice将会记录所有集群的后端Pod端点（IP 端口）。EndpointSlice是k8s内置对象，kube-proxy会负责将EndpointSlice对象记录的转发规则添加到IPVS或iptables中以实现服务的访问。
        * #### 冲突检查
            5个集群都要暴露同一个Service，但有些服务使用了clusterIP，有些没有，这点是需要做检查的。参阅 https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/1645-multi-cluster-services-api#exporting-services
            ```yaml
            apiVersion: multicluster.k8s.io/v1alpha1
            kind: ServiceExport
            metadata:
            name: my-svc
            namespace: my-ns
            status:
            conditions:
            - type: Ready
                status: "True"
                lastTransitionTime: "2020-03-30T01:33:51Z"
            - type: InvalidService
                status: "False"
                lastTransitionTime: "2020-03-30T01:33:55Z"
            - type: Conflict
                status: "True"
                lastTransitionTime: "2020-03-30T01:33:55Z"
                message: "Conflicting type. Using \"ClusterSetIP\" from oldest service export in \"cluster-1\". 2/5 clusters disagree."
            ```

    * #### ServiceImport
        * #### 解释
            用来表示某个集群主动导入cluster上的某个Service作为两个集群之间的跨集群服务。

            这个和k8s传统的Service非常类似。ServiceImport 和 ServiceExport 是相关联的，ServiceImport将会引用ServiceExport。

            ServiceImport 将会被  mcs-controller 的实现程序管理。


        * #### 引用服务
            要使用集群集服务，应使用与多集群服务关联的域名。当 mcs-controller 看到一个 ServiceExport 时，会在每个导入集群中引入一个 ServiceImport 来代表导入的服务。用户将访问 ServiceImport 上记录的 clusterSetIP 来最终路由到目标后端。

            一个 ServiceImport 就是一个 Service 含有其他集群的Endpoints，有以下三种情况：
            * 这个服务对应的Endpoints运行在其他集群上，不在本集群
            * 这个服务对应的Endpoints 即包含其他集群的Endpoints，也包含自己集群上的Endpoints
            * 这个服务对饮的Endpoints运行在本集群，但暴露给其他集群引用

            一个多集群服务将只会被含有相同命名空间的集群引用。当然实现 mcs-controller 的程序可以自己决定到底要不要自动创建命名空间到没有该命名空间的集群。

            由于 ServiceImport 可能在集群中产生广泛的潜在影响，因此不应允许非集群管理员用户创建或修改 ServiceImport 资源。 mcs-controller 应该单独负责 ServiceImport 的生命周期。

            当 ServiceImport 引用多个 EndpointSlices 时，加上 multicluster.kubernetes.io/service-name label给 EndpointSlices。用于检查 ServiceImport 到底引用了哪些 EndpointSlices 。

            每一个 EndpointSlice 同样带有一个label multicluster.kubernetes.io/source-cluster 表示自己来自哪个集群。

            EndpointSlices 数据和刚拿到时数据的一致性不保证，这服药 mcs-controller 负责管理 EndpointSlices的一致性，也就是拿 informer watch 去监听数据变化并更新。

        * #### 路由
            ServiceImport对象有.spec.ips来表示VIP，这个VIP将映射所有EndpointSlice后端地址。当前type=ClusterSetIP才行。

        * #### 行为
            * #### kind Service 的 type
                当kind Service的type为下列类型的时候 ServiceImport 将产生的行为
                * #### ClusterIP
                    就像传统的 Service 和 Endpoint 一样，将所有的 EndpointSlice 和 ServiceImport 的 clusterset IP 关联起来，访问该 clusterset IP 自动负载均衡到 EndpointSlice 对应的后端。前置条件是 ServiceImport 的 type=ClusterSetIP
                * #### ClusterIP=None
                    当Service是无头服务时，就像传统的 Service 和 Endpoint 的处理， ServiceImport 不在产生 VIP ，查询DNS的时候以 AAA 记录返回所有 EndpointSlice 对应的后端Pod地址
                * #### NodePort 和 LoadBalancer
                    在这种情况下，创建出来的 ServiceImport 不会产生打开 nodePort 端口的动作，和service一样 ServiceImport 会产生 VIP，也就是clusterSetIP ，将只会添加 VIP 到ipvs之类的路由上罢了。

                * #### ExternalName
                    导出 ExternalName 服务没有意义。 它们不能与其他导出合并，而且它似乎只会使部署复杂化，甚至试图将它们延伸到集群中。 相反，应该在每个集群中单独创建常规 ExternalName 类型的服务。 如果为 ExternalName 服务创建了 ServiceExport，则会在导出时设置 InvalidService 条件。
            * #### ClusterSetIP
                一个有头的 ServiceImport 总是期望得到一个 IP 地址（VIP），这个IP会被引用了跨集群服务的集群内的Pod访问，通常是单个IP，也可能是多个IP。pod请求这个IP将会被路由到本集群或其他集群的后端Pod上。

                注意：本文档不讨论 NetworkPolicy，它目前不能用于描述适用于多集群服务的基于选择器的策略。
            * #### DNS
                可选技术，但推荐。

                当一个 ServiceExport 被创建后，将产生一个可访问的域名 ```<service>.<ns>.svc.clusterset.local ```

                将根据 ServiceExport 引用的 服务是 ClusterSetIP 还是 无头服务产生不同的操作。
                * #### ClusterSetIP 
                    和传统k8s Service的机制是一样的，却别在于路由的后端超出本集群。在引用该服务对应域名的集群中查询这个域名时将返回clusterSetIP，请求该IP将会有可能随机（假如是RR模式的ipvs）路由到每个集群的Pod上。
                * #### Headless services
                    和传统k8s Service的机制是一样的，却别在于路由的后端超出本集群。在引用该服务对应域名的集群中查询这个域名时将返回DNS AAA 记录，DNS查询结果中将返回所有的EndpointSlice对应的PodIP，应用程序将需要自己做负载均衡，参阅 https://github.com/hakur/memory/blob/master/%E5%BB%BA%E8%AE%AE.md#%E5%9F%BA%E4%BA%8Ek8s-serviceipvs%E6%96%B9%E6%A1%88。