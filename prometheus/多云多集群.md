## 多云
* 即会将kubernetes和途灵系统部署到aws或者阿里云上。
## 多集群
* 一个云商之内部署多套kubernetes集群。

## 监控数据的采集
* Prometheus作为采集器，不能再作为查询器（稍后解释为什么不能作为查询，除非修改源码的那段代码，见于Prometheus external label坑）。
    * Prometheus external label坑
        远程读取会使用external_labels作为label匹配条件

        prometheus2.14版本见于 /storage/remote/read.go#70 插入fmt.Println(query.String())

        打印一下查询条件就能看到
        
        这回引发问题：一个Prometheus在webgui上操作只能搜到自己的数据，搜不到整个集群其他Prometheus的数据。

        如果删除这段自动添加externel label作为查询条件的代码，则可读取全部的数据了。
<br />
<br />
<br />

## 一个云内多个集群数据的存储
* ### 如何区分集群数据
    Prometheus需要追加external_labels配置项在global节点。external_labels的值如kube_cluster: "cluster-a"。这样可以在grafana的查询表达式中作为查询条件，只看这个集群的数据。因此集群的名称在确定后，不可再更改以免Prometheus查不到历史数据。
* ### Prometheus的数据发往何处汇聚并归档
    使用cortex或者m3db来解决会聚存储问题，他们都兼容Prometheus http restful api，因此可以完美搭配grafana使用。
* ### 数据存储时间和归档数据的查询
    使用cortex或m3db之后

* ### 方案缺陷
    文档稀少，需要对源码深入研究，victoria metrics 也逃不过研究源码。

* ### 不用thanos和联邦的原因
    为什么要用cortex和m3db 而不是prometheus联邦与thanos(我更喜欢称呼灭霸)？因为thanos的主要功能为在查询时聚合多个单机prometheus的查询结果数据到一起并返回，数据量超大且缓慢无比。而Prometheus联邦这个事，如果用联邦能解决，我宁可只用一个Prometheus来存储所有的数据。
    
    cortex和m3db都兼容Prometheus api 并且附带分片多副本和存储功能，并支持老旧数据S3存储归档查询。其中cortex还支持 多个prometheus 推送相同数据只取其中一份（后面介绍）。

### grafana面板更改
* 用户会在面板的导航栏依次选择 数据源(即云) -> 集群名称(这套云内的某个kubernetes数据) -> 其他条件。这些条件 都是grafana面板上prometheus的查询条件，如图所示。导航栏上的$datasource 变量可以在promql中使用。
    图xxxxxxxxxxxxxxxx








### 大改方案
    cortex + prometheus + alertmanager

    proemtheus -> cortex
    cortext -> alertmanager，Prometheus不在持有alert rule 和 recording rule，转交cortex来管理这两个rule。

### 小改方案
    cortext + prometheus + alertmanager

    prometheus（writer角色） 写数据-> cortex
    prometheus（reader角色） 读取数据 -> cortex,同时发送告警，这个Prometheus主要是删除了remote read 会自动添加external label作为查询条件的逻辑


每一种云 一个cortex集群，不同云不同区域直接网速差异很大，一个地区一个云走同一个内网防止Prometheus推送的流量走公网过导致速度出现问题，3个grafana，n个prometheus各自采集不同的任务如果量大，量小的时候这个集群部署两个Prometheus采集相同的任务就够了，一个或多个修改过external label代码的Prometheus来作为查询reader，这样可以继续保留现有的Prometheus recording 和 alert rule。

Prometheus 实例需要给与单独的节点保证数据能支持传送而不会由于 其他pod用完了资源节点崩溃 ，亦或者其他pod用光了 这个虚拟机的内网带宽。